# ml-model-mini-batch-gradient-descent
I learned about mini-batch gradient descent and its application in optimizing machine learning models. This method involves updating model parameters using small batches of data rather than the entire dataset in each iteration. It improves computational efficiency, facilitates handling large datasets, and introduces stochasticity that can aid in escaping local minima during model training. Understanding mini-batch gradient descent enhances my ability to effectively train and optimize machine learning algorithms, particularly in scenarios involving big data and complex models.
